# SFT Training Configuration
# For training on external GPU providers (RunPod, Modal, etc.)

# Model Configuration
model:
  name: "deepseek-ai/deepseek-coder-7b-base-v1.5"
  # Alternatives:
  # - "codellama/CodeLlama-7b-hf"
  # - "Qwen/Qwen2.5-Coder-7B"
  # - "bigcode/starcoder2-7b"
  torch_dtype: "bfloat16"
  device_map: "auto"
  trust_remote_code: true

# Quantization (QLoRA)
quantization:
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA Configuration
lora:
  r: 32  # Rank
  lora_alpha: 64
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Training Arguments
training:
  output_dir: "./outputs/sft"
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8  # Effective batch size = 16
  learning_rate: 2.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0
  
  # Optimization
  optim: "paged_adamw_32bit"
  fp16: false
  bf16: true
  
  # Logging
  logging_steps: 10
  save_strategy: "steps"
  save_steps: 100
  eval_strategy: "steps"
  eval_steps: 100
  
  # Memory optimization
  gradient_checkpointing: true
  max_seq_length: 4096

# Dataset Configuration
dataset:
  name: "ScaleAI/SWE-bench_Pro"
  split: "test"  # SWE-Bench Pro only has test split
  text_field: "formatted_text"
  max_samples: null  # Use all samples, or set a number for debugging
  
# Prompt Template
prompt_template: |
  ### Task: Fix GitHub Issue
  
  Repository: {repo}
  
  ### Issue Description:
  {problem_statement}
  
  ### Instructions:
  Generate a unified diff patch that fixes the issue described above.
  The patch should be in standard unified diff format.
  
  ### Patch:
  {patch}

# HuggingFace Hub
hub:
  push_to_hub: true
  hub_model_id: "rahjeetee/swe-patch-sft"
  hub_private: false
